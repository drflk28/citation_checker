import requests
import re
import json
from typing import Dict, Any, List, Optional
from urllib.parse import quote
from bs4 import BeautifulSoup
import time


class RussianSourcesSearcher:
    """–†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""

    def __init__(self):
        self.rsl_base_url = "https://search.rsl.ru"
        self.cyberleninka_base_url = "https://cyberleninka.ru"
        self.elibrary_base_url = "https://elibrary.ru"

    def search_publication(self, query: str, original_text: str = "") -> Optional[Dict[str, Any]]:
        """–ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–µ–µ"""
        try:
            # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏
            publication_data = self._deep_analyze_bibliography(original_text)

            print(f"    üîç –ê–Ω–∞–ª–∏–∑: {publication_data['authors']} - {publication_data['title']}")

            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥
            result = self._find_concrete_publication(publication_data)

            if result:
                print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è: {result['source']}")
                return result
            else:
                print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é")
                return None

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return None

    def _deep_analyze_bibliography(self, text: str) -> Dict[str, Any]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏"""
        if not text:
            return {'authors': [], 'title': '', 'year': None, 'publisher': None}

        clean_text = re.sub(r'\s+', ' ', text.strip())

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ—Ö –∞–≤—Ç–æ—Ä–æ–≤
        authors = self._extract_all_authors(clean_text)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        title = self._extract_complete_title(clean_text, authors)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥
        year = self._extract_year(clean_text)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        publisher = self._extract_publisher(clean_text)

        return {
            'authors': authors,
            'title': title,
            'year': year,
            'publisher': publisher,
            'original_text': clean_text
        }

    def _extract_all_authors(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ—Ö –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏"""
        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø–∏—Å–∏ –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        author_section_match = re.match(r'^([^.‚Äî]+?)(?=\.|‚Äî|/)', text)
        if not author_section_match:
            return []

        author_section = author_section_match.group(1).strip()

        # –†–∞–∑–¥–µ–ª—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤
        authors = []
        author_parts = re.split(r',|\s+–∏\s+', author_section)

        for part in author_parts:
            part = part.strip()
            if part and len(part) > 2:
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                part = re.sub(r'\s+', ' ', part)
                authors.append(part)

        return authors

    def _extract_complete_title(self, text: str, authors: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        # –£–±–∏—Ä–∞–µ–º —Å–µ–∫—Ü–∏—é –∞–≤—Ç–æ—Ä–æ–≤
        text_without_authors = text
        if authors:
            first_author = authors[0]
            # –ò—â–µ–º –∫–æ–Ω–µ—Ü –∞–≤—Ç–æ—Ä—Å–∫–æ–π —Å–µ–∫—Ü–∏–∏ (—Ç–æ—á–∫–∞, —Ç–∏—Ä–µ, –¥–≤–æ–µ—Ç–æ—á–∏–µ)
            author_end_match = re.search(r'^[^.‚Äî]*[.‚Äî]', text)
            if author_end_match:
                text_without_authors = text[len(author_end_match.group(0)):].strip()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        title_match = re.search(r'^([^.‚Äî]*?)(?=\.\s*[–ê-–ØA-Z]|‚Äî|\s*\d{4}|$)', text_without_authors)
        if title_match:
            title = title_match.group(1).strip()
            if title and len(title) > 10:
                return self._clean_title(title)

        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        return self._clean_title(text_without_authors[:100])

    def _find_concrete_publication(self, publication_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ò—â–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é —á–µ—Ä–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_query = self._create_search_query(publication_data)

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞
            strategies = [
                self._try_rsl_concrete_search,
                self._try_cyberleninka_concrete_search,
                self._try_elibrary_concrete_search
            ]

            for strategy in strategies:
                result = strategy(publication_data, search_query)
                if result and result.get('url'):
                    return result

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")

        return None

    def _try_rsl_concrete_search(self, publication_data: Dict[str, Any], search_query: str) -> Optional[Dict[str, Any]]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å –≤ –†–ì–ë"""
        try:
            encoded_query = quote(search_query)
            search_url = f"{self.rsl_base_url}/ru/search?q={encoded_query}"

            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –†–ì–ë
            # –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Å—ã–ª–∫—É, –Ω–æ —Å –ø–æ–º–µ—Ç–∫–æ–π —á—Ç–æ —ç—Ç–æ –ø–æ–∏—Å–∫

            return {
                'source': 'rsl',
                'title': publication_data['title'],
                'authors': publication_data['authors'],
                'year': publication_data['year'],
                'publisher': publication_data['publisher'],
                'url': search_url,
                'is_search_link': True,  # –ü–æ–º–µ—á–∞–µ–º —á—Ç–æ —ç—Ç–æ –ø–æ–∏—Å–∫–æ–≤–∞—è —Å—Å—ã–ª–∫–∞
                'confidence': 0.7,
                'description': f'–ü–æ–∏—Å–∫ –≤ –†–ì–ë: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–∑–¥–∞–Ω–∏—è'
            }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –†–ì–ë: {e}")
            return None

    def _try_cyberleninka_concrete_search(self, publication_data: Dict[str, Any], search_query: str) -> Optional[
        Dict[str, Any]]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é –≤ CyberLeninka"""
        try:
            # –î–ª—è —Å—Ç–∞—Ç–µ–π –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é
            if any(keyword in publication_data['original_text'].lower() for keyword in ['—Å—Ç–∞—Ç—å—è', '–∂—É—Ä–Ω–∞–ª']):
                encoded_query = quote(search_query)
                search_url = f"{self.cyberleninka_base_url}/search?q={encoded_query}"

                return {
                    'source': 'cyberleninka',
                    'title': publication_data['title'],
                    'authors': publication_data['authors'],
                    'year': publication_data['year'],
                    'journal': '–ù–∞—É—á–Ω—ã–π –∂—É—Ä–Ω–∞–ª',
                    'url': search_url,
                    'is_search_link': True,
                    'confidence': 0.6,
                    'description': f'–ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π'
                }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ CyberLeninka: {e}")

        return None

    def _try_elibrary_concrete_search(self, publication_data: Dict[str, Any], search_query: str) -> Optional[
        Dict[str, Any]]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é –≤ eLibrary"""
        try:
            encoded_query = quote(search_query)
            search_url = f"{self.elibrary_base_url}/search.asp?phrase={encoded_query}"

            return {
                'source': 'elibrary',
                'title': publication_data['title'],
                'authors': publication_data['authors'],
                'year': publication_data['year'],
                'publisher': publication_data['publisher'],
                'url': search_url,
                'is_search_link': True,
                'confidence': 0.5,
                'description': f'–ü–æ–∏—Å–∫ –≤ –Ω–∞—É—á–Ω–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ'
            }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ eLibrary: {e}")
            return None

    def _create_search_query(self, publication_data: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        authors_str = ' '.join(publication_data['authors']) if publication_data['authors'] else ""
        title = publication_data['title']

        if authors_str and title:
            return f"{authors_str} {title}"
        elif title:
            return title
        else:
            return publication_data['original_text'][:100]

    def _extract_year(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ–¥"""
        match = re.search(r'\b(19|20)\d{2}\b', text)
        return match.group(0) if match else None

    def _extract_publisher(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ"""
        # –ò—â–µ–º –ø–æ—Å–ª–µ —Ç–∏—Ä–µ –∏ –≥–æ—Ä–æ–¥–∞
        patterns = [
            r'‚Äî\s*[^:]*:\s*([^.,]+?)(?=\.|,|\s*\d|$)',
            r'‚Äî\s*([^.,]+?)(?=\.|,|\s*\d|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                publisher = match.group(1).strip()
                if len(publisher) > 3:
                    return publisher

        return None

    def _clean_title(self, title: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ"""
        if not title:
            return ""

        # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
        patterns_to_remove = [
            r'\/\/.*$',
            r'‚Äî.*$',
            r'\.‚Äî.*$',
            r'\[.*?\]',
            r'\(.*?\)',
            r'\b(–∏–∑–¥-–≤–æ|–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ|—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è|—Å—Ç–∞—Ç—å—è)\b.*$',
        ]

        clean_title = title
        for pattern in patterns_to_remove:
            clean_title = re.sub(pattern, '', clean_title, flags=re.IGNORECASE)

        clean_title = re.sub(r'\s+', ' ', clean_title).strip()

        return clean_title