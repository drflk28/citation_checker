import re
import string
from typing import Dict, List, Tuple, Optional, Any, Set
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)


class FixedSemanticCitationMatcher:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è: –∏—Å–∫–ª—é—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –ø–æ–∏—Å–∫–∞
    """

    def __init__(self, language: str = 'russian'):
        self.language = language

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF —Å —Ä—É—Å—Å–∫–∏–º–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º–∏ (–∫–∞–∫ —É –≤–∞—Å)
        russian_stop_words = {
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ',
            '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞',
            '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç',
            '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏',
            '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å',
            '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏',
            '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞',
            '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç',
            '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º',
            '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å',
            '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞',
            '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å',
            '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º',
            '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è',
            '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
        }

        self.vectorizer = TfidfVectorizer(
            max_features=7000,
            stop_words=list(russian_stop_words),
            ngram_range=(1, 3),
            min_df=1,
            max_df=0.9,
            sublinear_tf=True
        )

    def preprocess_text(self, text: str, preserve_keywords: bool = True) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–∫–∞–∫ —É –≤–∞—Å)"""
        if not text:
            return ""

        text = text.lower()

        if preserve_keywords:
            text = re.sub(r'(\w+)-(\w+)', r'\1_\2', text)
            text = re.sub(r'[^\w\s.,!?;:()"\'_\-]', ' ', text)
            text = text.replace('_', '-')
        else:
            text = re.sub(r'[^\w\s]', ' ', text)

        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_key_phrases(self, text: str, max_phrases: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ (–∫–∞–∫ —É –≤–∞—Å)"""
        if not text:
            return []

        text_clean = self.preprocess_text(text, preserve_keywords=True)
        words = text_clean.split()

        russian_stop_words = {
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ',
            '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞',
            '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç'
        }

        filtered_words = []
        for w in words:
            w_clean = w.strip('.,!?;:()"\'')
            if w_clean and w_clean not in russian_stop_words and len(w_clean) > 2:
                filtered_words.append(w_clean)

        if len(filtered_words) < 2:
            return filtered_words

        phrases = []
        phrases.extend([w for w in filtered_words if len(w) > 3])

        for i in range(len(filtered_words) - 1):
            bigram = f"{filtered_words[i]} {filtered_words[i + 1]}"
            if len(bigram) > 5:
                phrases.append(bigram)

        for i in range(len(filtered_words) - 2):
            trigram = f"{filtered_words[i]} {filtered_words[i + 1]} {filtered_words[i + 2]}"
            if len(trigram) > 8:
                phrases.append(trigram)

        phrase_counter = Counter(phrases)
        sorted_phrases = sorted(
            phrase_counter.items(),
            key=lambda x: (x[1], len(x[0].split())),
            reverse=True
        )

        return [phrase for phrase, count in sorted_phrases[:max_phrases]]

    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (–∫–∞–∫ —É –≤–∞—Å)"""
        if not text1 or not text2:
            return 0.0

        text1_clean = self.preprocess_text(text1)
        text2_clean = self.preprocess_text(text2)

        if len(text1_clean.split()) < 5 or len(text2_clean.split()) < 5:
            return self._calculate_jaccard_similarity(text1_clean, text2_clean)

        try:
            if hasattr(self.vectorizer, 'vocabulary_'):
                vec1 = self.vectorizer.transform([text1_clean])
                vec2 = self.vectorizer.transform([text2_clean])
            else:
                tfidf_matrix = self.vectorizer.fit_transform([text1_clean, text2_clean])
                vec1 = tfidf_matrix[0:1]
                vec2 = tfidf_matrix[1:2]

            similarity = cosine_similarity(vec1, vec2)[0][0]

            if len(text1_clean.split()) < 10 or len(text2_clean.split()) < 10:
                jaccard = self._calculate_jaccard_similarity(text1_clean, text2_clean)
                similarity = 0.6 * similarity + 0.4 * jaccard

            return float(similarity)
        except Exception as e:
            logger.error(f"Error calculating similarity: {e}")
            return self._calculate_jaccard_similarity(text1_clean, text2_clean)

    def _calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """Jaccard similarity (–∫–∞–∫ —É –≤–∞—Å)"""
        words1 = {w for w in text1.split() if len(w) > 2}
        words2 = {w for w in text2.split() if len(w) > 2}

        if not words1 or not words2:
            return 0.0

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0

    def find_semantic_matches(self, citation_text: str, source_content: str,
                              source_metadata: Optional[Dict[str, Any]] = None,
                              context_window: int = 500) -> List[Dict[str, Any]]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
        """
        if not citation_text or not source_content:
            return []

        # üîç –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –û–¢–õ–ê–î–ö–ê
        print(f"\nüîç find_semantic_matches –ø–æ–ª—É—á–∏–ª:")
        print(f"   –¶–∏—Ç–∞—Ç–∞: {citation_text[:100]}...")
        print(f"   –†–∞–∑–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {len(source_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_content[:200]}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Ü–∏—Ç–∞—Ç—ã
        key_phrases = self.extract_key_phrases(citation_text)
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã: {key_phrases[:10]}")

        # –í–ê–ñ–ù–û: —Å–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã, –∞ –ø–æ—Ç–æ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–µ–≤
        paragraphs = self._split_into_smart_paragraphs(source_content)

        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.debug(f"üìë –°–æ–∑–¥–∞–Ω–æ {len(paragraphs)} –∞–±–∑–∞—Ü–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
        for idx, para in enumerate(paragraphs[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            logger.debug(f"   –ê–±–∑–∞—Ü {idx + 1}: {len(para.split())} —Å–ª–æ–≤, {len(para)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.debug(f"      {para[:100]}...")

        matches = []

        for i, paragraph in enumerate(paragraphs):
            # –ü–†–û–ü–£–°–ö–ê–ï–ú –ö–û–†–û–¢–ö–ò–ï –°–¢–†–û–ö–ò (–º–µ–Ω—å—à–µ 50 —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ 10 —Å–ª–æ–≤)
            word_count = len(paragraph.split())
            char_count = len(paragraph)

            if word_count < 10 or char_count < 50:
                logger.debug(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç ({word_count} —Å–ª–æ–≤, {char_count} —Å–∏–º–≤–æ–ª–æ–≤)")
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = self.calculate_semantic_similarity(citation_text, paragraph)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
            key_phrase_matches = []
            paragraph_lower = paragraph.lower()

            for phrase in key_phrases:
                phrase_lower = phrase.lower()
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (1-2 –±—É–∫–≤—ã)
                if len(phrase_lower) < 3:
                    continue
                if phrase_lower in paragraph_lower:
                    key_phrase_matches.append(phrase)

            # –ï—Å–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if word_count < 20 and len(key_phrase_matches) >= 2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                if self._looks_like_title(paragraph):
                    logger.debug(f"üö´ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: {paragraph[:50]}...")
                    continue

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            if similarity > 0.2 or len(key_phrase_matches) >= 2:
                # üîç –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                print(f"\n‚úÖ –ù–ê–ô–î–ï–ù–û –°–û–í–ü–ê–î–ï–ù–ò–ï #{len(matches) + 1}:")
                print(f"   –ê–±–∑–∞—Ü {i + 1} ({word_count} —Å–ª–æ–≤, —Å—Ö–æ–∂–µ—Å—Ç—å {similarity:.3f})")
                print(f"   –ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑: {key_phrase_matches[:5]}")
                print(f"   –¢–µ–∫—Å—Ç: {paragraph[:200]}...")

                matches.append({
                    'fragment': paragraph[:500] + ('...' if len(paragraph) > 500 else ''),
                    'fragment_full': paragraph,
                    'similarity_score': similarity,
                    'raw_similarity': similarity,
                    'key_phrase_matches': key_phrase_matches,
                    'key_phrase_count': len(key_phrase_matches),
                    'word_count': word_count,
                    'char_count': char_count,
                    'fragment_index': i
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤ (—á–µ–º –¥–ª–∏–Ω–Ω–µ–µ, —Ç–µ–º –ª—É—á—à–µ) –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        for match in matches:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ
            penalty = self._penalize_title_paragraph(match['fragment_full'])
            match['penalized_score'] = match['similarity_score'] * (1 - penalty)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —à—Ç—Ä–∞—Ñ–æ–≤–∞–Ω–Ω–æ–º—É score
        matches.sort(key=lambda x: x['penalized_score'], reverse=True)

        print(f"\nüìä –í–°–ï–ì–û –ù–ê–ô–î–ï–ù–û –°–û–í–ü–ê–î–ï–ù–ò–ô: {len(matches)}")
        if matches:
            print(f"üèÜ –õ–£–ß–®–ï–ï –°–û–í–ü–ê–î–ï–ù–ò–ï:")
            print(f"   –°–ª–æ–≤: {matches[0]['word_count']}, —Å—Ö–æ–∂–µ—Å—Ç—å: {matches[0]['similarity_score']:.3f}")
            print(f"   –§—Ä–∞–∑—ã: {matches[0]['key_phrase_matches'][:5]}")
            print(f"   –¢–µ–∫—Å—Ç: {matches[0]['fragment'][:200]}...")

        return matches

    def _looks_like_title(self, text: str) -> bool:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        """
        if not text or len(text) < 10:
            return True

        text_lower = text.lower()

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_indicators = [
            '–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–ø–∞—Ä–∞–≥—Ä–∞—Ñ', '¬ß',
            '—É—á–µ–±–Ω–∏–∫', '–ø–æ—Å–æ–±–∏–µ', '–∏–∑–¥–∞–Ω–∏–µ', '—Ç–æ–º',
            '–≤–≤–µ–¥–µ–Ω–∏–µ', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ',
            '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Å–ø–∏—Å–æ–∫', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
            '–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è', 'references', 'index',
            '–∞–Ω–Ω–∞', '–º–∏—Ö–∞–π–ª–æ–≤–Ω–∞', '–ª–æ–ø–∞—Ä–µ–≤–∞', '–∞–≤—Ç–æ—Ä'
        ]

        for indicator in title_indicators:
            if indicator in text_lower:
                return True

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –Ω–∞–ø–∏—Å–∞–Ω –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏
        if len(text) < 100 and text.isupper():
            return True

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã
        if re.search(r'–≥–ª–∞–≤–∞\s+\d+|^\d+\.\d+', text_lower):
            return True

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –∫–æ—Ä–æ—Ç–∫–∏–π
        if len(text) < 100 and not any(p in text for p in '.!?;:'):
            return True

        return False

    def _split_into_smart_paragraphs(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º (—Ä–µ–∞–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã)
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫ (—Ä–µ–∞–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã)
        raw_paragraphs = re.split(r'\n\s*\n', text)

        paragraphs = []
        for para in raw_paragraphs:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞
            lines = para.split('\n')
            cleaned_lines = [line.strip() for line in lines if line.strip()]
            if cleaned_lines:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
                paragraph_text = ' '.join(cleaned_lines)
                paragraphs.append(paragraph_text)

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –∫–∞–∫ fallback
        if len(paragraphs) <= 1:
            lines = text.split('\n')
            lines = [line.strip() for line in lines if line.strip()]

            if not lines:
                return []

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–º—ã—Å–ª—É (–∫–∞–∂–¥—ã–µ 5-7 —Å—Ç—Ä–æ–∫)
            paragraphs = []
            current_paragraph = []

            for line in lines:
                current_paragraph.append(line)
                # –ü—Ä–∏–∑–Ω–∞–∫ –∫–æ–Ω—Ü–∞ –∞–±–∑–∞—Ü–∞: —Å—Ç—Ä–æ–∫–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π –∏ —Å–ª–µ–¥—É—é—â–∞—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                if line.endswith(('.', '!', '?')) and len(current_paragraph) >= 3:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []

            if current_paragraph:
                if paragraphs and len(current_paragraph) < 3:
                    paragraphs[-1] = paragraphs[-1] + ' ' + ' '.join(current_paragraph)
                else:
                    paragraphs.append(' '.join(current_paragraph))

        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"\nüìë –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê –ê–ë–ó–ê–¶–´:")
        for i, para in enumerate(paragraphs):
            words = len(para.split())
            chars = len(para)
            print(f"   –ê–±–∑–∞—Ü {i + 1}: {words} —Å–ª–æ–≤, {chars} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"      {para[:100]}...")

        return paragraphs

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã (–º–∏–Ω–∏–º—É–º 50 —Å–ª–æ–≤ –∏–ª–∏ 300 —Å–∏–º–≤–æ–ª–æ–≤)
        """
        lines = text.split('\n')
        lines = [line.strip() for line in lines if line.strip()]

        if not lines:
            return []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ –∞–±–∑–∞—Ü—ã
        paragraphs = []
        current_paragraph = []
        current_length = 0

        for line in lines:
            current_paragraph.append(line)
            current_length += len(line)

            # –ï—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞–±–∑–∞—Ü
            if current_length >= 300 or len(current_paragraph) >= 5:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
                current_length = 0

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
        if current_paragraph:
            # –ï—Å–ª–∏ –æ—Å—Ç–∞—Ç–æ–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É –∞–±–∑–∞—Ü—É
            if paragraphs and current_length < 100:
                paragraphs[-1] = paragraphs[-1] + ' ' + ' '.join(current_paragraph)
            else:
                paragraphs.append(' '.join(current_paragraph))

        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.debug(f"üìë –°–æ–∑–¥–∞–Ω–æ {len(paragraphs)} –∞–±–∑–∞—Ü–µ–≤:")
        for i, para in enumerate(paragraphs):
            logger.debug(f"   –ê–±–∑–∞—Ü {i + 1}: {len(para.split())} —Å–ª–æ–≤, {len(para)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.debug(f"      {para[:100]}...")

        return paragraphs

    def _remove_common_metadata_words(self, citation_text: str, paragraph: str) -> tuple:
        """
        –£–¥–∞–ª—è–µ—Ç –∏–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
        """
        common_metadata_words = {
            '–ª–æ–ø–∞—Ä–µ–≤–∞', '–±–∏–∑–Ω–µ—Å', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '—É—á–µ–±–Ω–∏–∫', '–≤—É–∑–æ–≤', '–∏–∑–¥–∞–Ω–∏–µ',
            '–ø–µ—Ä–µ—Ä–∞–±', '–¥–æ–ø', '–∞–Ω–Ω–∞', '–º–∏—Ö–∞–π–ª–æ–≤–Ω–∞', '–≥–ª–∞–≤–∞', '—Ä–µ–∑—é–º–µ', '–ø—Ä–æ–µ–∫—Ç–∞'
        }

        # –û—á–∏—â–∞–µ–º —Ü–∏—Ç–∞—Ç—É –æ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        citation_words = set(citation_text.lower().split())
        citation_filtered = citation_words - common_metadata_words

        # –û—á–∏—â–∞–µ–º –∞–±–∑–∞—Ü –æ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        paragraph_words = set(paragraph.lower().split())
        paragraph_filtered = paragraph_words - common_metadata_words

        return citation_filtered, paragraph_filtered

    def _find_position_in_source(self, full_text: str, fragment: str) -> Dict[str, int]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ü–∏—é —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ"""
        try:
            start_pos = full_text.find(fragment[:100])  # –ò—â–µ–º –Ω–∞—á–∞–ª–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
            if start_pos >= 0:
                return {
                    'start': start_pos,
                    'end': start_pos + len(fragment)
                }
        except:
            pass
        return {'start': 0, 'end': 0}

    def verify_citation_in_source(self, citation_data: Dict[str, Any],
                                  source_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –∏—Å–∫–ª—é—á–∞—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        citation_text = citation_data.get('full_paragraph', '') or citation_data.get('text', '')
        citation_context = citation_data.get('context', '')
        source_content = source_data.get('full_content', '')

        if not source_content:
            return {
                'verified': False,
                'confidence': 0,
                'reason': '–¢–µ–∫—Å—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏',
                'matches': []
            }

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç —Ü–∏—Ç–∞—Ç—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        full_citation_text = f"{citation_text} {citation_context}".strip()
        if not full_citation_text:
            full_citation_text = citation_text

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        source_metadata = {
            'title': source_data.get('title', ''),
            'authors': source_data.get('authors', []),
            'publisher': source_data.get('publisher', ''),
            'year': source_data.get('year', '')
        }

        # ========== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–ê ==========

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, —É–¥–∞–ª—è—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        clean_content = self._extract_main_content(source_content, source_metadata)

        # 2. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã (–Ω–µ –ø–æ —Å–ª–æ–≤–∞–º, –∞ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ)
        paragraphs = self._split_into_paragraphs(clean_content)

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Ü–∏—Ç–∞—Ç—ã
        key_phrases = self.extract_key_phrases(full_citation_text, max_phrases=15)

        # 4. –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        stop_words = self._get_stop_words_from_metadata(source_metadata)

        # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∞–±–∑–∞—Ü
        best_match = None
        best_score = 0
        all_matches = []

        for paragraph in paragraphs:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
            if len(paragraph.split()) < 15 or len(paragraph) < 100:
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–±–∑–∞—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if self._looks_like_title(paragraph) or self._is_metadata_paragraph(paragraph):
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = self.calculate_semantic_similarity(full_citation_text, paragraph)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ (–∏—Å–∫–ª—é—á–∞—è —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
            paragraph_lower = paragraph.lower()
            meaningful_phrases = []

            for phrase in key_phrases:
                phrase_lower = phrase.lower()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ—Ä–∞–∑—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
                if any(stop_word in phrase_lower for stop_word in stop_words):
                    continue
                if phrase_lower in paragraph_lower:
                    meaningful_phrases.append(phrase)

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            phrase_score = len(meaningful_phrases) / max(len(key_phrases), 1) * 0.4
            similarity_score = similarity * 0.6

            total_score = phrase_score + similarity_score

            # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –∞–±–∑–∞—Ü–∞ (—á–µ–º –¥–ª–∏–Ω–Ω–µ–µ, —Ç–µ–º –≤–µ—Ä–æ—è—Ç–Ω–µ–µ, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç)
            length_bonus = min(len(paragraph.split()) / 500, 0.2)
            total_score += length_bonus

            match_info = {
                'text': paragraph[:500] + ('...' if len(paragraph) > 500 else ''),
                'full_text': paragraph,
                'similarity': similarity,
                'phrase_matches': meaningful_phrases,
                'phrase_count': len(meaningful_phrases),
                'score': total_score,
                'word_count': len(paragraph.split())
            }

            all_matches.append(match_info)

            if total_score > best_score:
                best_score = total_score
                best_match = match_info

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        all_matches.sort(key=lambda x: x['score'], reverse=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if best_match and best_score > 0.3:  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = min(best_score * 100, 95)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if confidence > 70:
                level = 'high'
            elif confidence > 50:
                level = 'medium'
            else:
                level = 'low'

            return {
                'verified': True,
                'confidence': round(confidence, 1),
                'verification_level': level,
                'reason': f'–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {round(confidence, 1)}%',
                'best_match': {
                    'text': best_match['text'],
                    'similarity': best_match['similarity'],
                    'key_phrases_matched': best_match['phrase_matches'],
                    'key_phrase_count': best_match['phrase_count'],
                    'word_count': best_match['word_count']
                },
                'all_matches': [
                    {
                        'text': m['text'][:200] + ('...' if len(m['text']) > 200 else ''),
                        'similarity': m['similarity'],
                        'key_phrases': m['phrase_matches'][:3],
                        'score': m['score']
                    }
                    for m in all_matches[:3]
                ],
                'analysis_details': {
                    'citation_length': len(full_citation_text),
                    'source_length': len(source_content),
                    'clean_content_length': len(clean_content),
                    'total_matches_found': len(all_matches),
                    'key_phrases_extracted': len(key_phrases)
                }
            }

        return {
            'verified': False,
            'confidence': 0,
            'reason': '–°–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞',
            'matches': []
        }

    def _extract_main_content(self, source_content: str, metadata: Dict[str, Any]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç, —É–¥–∞–ª—è—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        if not source_content:
            return ""

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        stop_words = self._get_stop_words_from_metadata(metadata)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
        lines = source_content.split('\n')
        lines = [line.strip() for line in lines if line.strip()]

        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        content_start = 0
        for i, line in enumerate(lines):
            line_lower = line.lower()

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
            words = line_lower.split()
            if words and all(word in stop_words for word in words):
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if self._looks_like_title(line):
                continue

            # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–∞—è –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            if len(line) > 50 and any(p in line for p in '.!?;:'):
                content_start = i
                break

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–∞—á–∞–ª–æ –≥–ª–∞–≤—ã/—Ä–∞–∑–¥–µ–ª–∞
            if re.search(r'–≥–ª–∞–≤–∞\s+\d+|^\d+\.\d+', line_lower):
                content_start = i
                break

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏
        if content_start > 0:
            return '\n'.join(lines[content_start:])
        return source_content

    def _get_stop_words_from_metadata(self, metadata: Dict[str, Any]) -> set:
        """
        –°–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        stop_words = set()

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        if metadata.get('title'):
            title_words = re.findall(r'\w+', metadata['title'].lower())
            stop_words.update(title_words)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–º–∏–ª–∏–∏ –∞–≤—Ç–æ—Ä–æ–≤
        if metadata.get('authors'):
            for author in metadata['authors']:
                if isinstance(author, str):
                    author_words = re.findall(r'\w+', author.lower())
                    stop_words.update(author_words)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞
        if metadata.get('publisher'):
            publisher_words = re.findall(r'\w+', metadata['publisher'].lower())
            stop_words.update(publisher_words)

        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–¥
        if metadata.get('year'):
            stop_words.add(str(metadata['year']))

        return stop_words

    def _split_into_paragraphs(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º –∏–ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º (—Ä–µ–∞–ª—å–Ω—ã–µ –∞–±–∑–∞—Ü—ã)
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]

        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –±–æ–ª—å—à–µ 1 –∞–±–∑–∞—Ü–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        if len(paragraphs) > 1:
            return paragraphs

        # –ò–Ω–∞—á–µ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º
        sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        paragraphs = []
        current_paragraph = []
        current_length = 0

        for sentence in sentences:
            current_paragraph.append(sentence)
            current_length += len(sentence)

            # –ï—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ (–±–æ–ª—å—à–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ 3+ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
            if current_length > 300 or len(current_paragraph) >= 3:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
                current_length = 0

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
        if current_paragraph:
            if paragraphs and current_length < 100:
                paragraphs[-1] = paragraphs[-1] + ' ' + ' '.join(current_paragraph)
            else:
                paragraphs.append(' '.join(current_paragraph))

        return paragraphs

    def _determine_verification_level(self, match: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        similarity = match.get('similarity_score', 0)
        key_phrases = match.get('key_phrase_count', 0)

        if similarity > 0.6 or (similarity > 0.4 and key_phrases >= 3):
            return 'high'
        elif similarity > 0.4 or key_phrases >= 2:
            return 'medium'
        elif similarity > 0.25 or key_phrases >= 1:
            return 'low'
        else:
            return 'very_low'

    def _is_metadata_paragraph(self, paragraph: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∞–±–∑–∞—Ü –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not paragraph or len(paragraph) < 20:
            return True

        paragraph_lower = paragraph.lower()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata_patterns = [
            r'^–∞–Ω–Ω–∞|–∞–ª–µ–∫—Å–∞–Ω–¥—Ä|–≤–ª–∞–¥–∏–º–∏—Ä|–∏–≤–∞–Ω|–ø–µ—Ç—Ä|—Å–µ—Ä–≥–µ–π|–¥–º–∏—Ç—Ä–∏–π',
            r'–ª–æ–ø–∞—Ä–µ–≤–∞|–∏–≤–∞–Ω–æ–≤|–ø–µ—Ç—Ä–æ–≤|—Å–∏–¥–æ—Ä–æ–≤|—Å–º–∏—Ä–Ω–æ–≤|–∫—É–∑–Ω–µ—Ü–æ–≤',
            r'—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|–∏–∑–¥–∞–Ω–∏–µ|–∏–∑–¥\.|–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ',
            r'–º–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏|–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ|—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∞–∫–∞–¥–µ–º–∏—è|–∏–Ω—Å—Ç–∏—Ç—É—Ç',
            r'—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥|–º–æ—Å–∫–≤–∞|–ª—ç—Ç–∏|—é—É—Ä–≥—É|–≤–ª–≥—É',
        ]

        for pattern in metadata_patterns:
            if re.search(pattern, paragraph_lower):
                return True

        return False

    def debug_paragraph_splitting(self, source_content: str):
        """
        –ü–æ–¥—Ä–æ–±–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã
        """
        print("\n" + "=" * 80)
        print("üîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –†–ê–ó–ë–ò–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê")
        print("=" * 80)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        print(f"\nüìÑ –ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢ ({len(source_content)} —Å–∏–º–≤–æ–ª–æ–≤):")
        print("-" * 40)
        lines = source_content.split('\n')
        for i, line in enumerate(lines):
            if line.strip():
                print(f"  {i + 1}: '{line}'")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç _split_into_smart_paragraphs
        print(f"\nüìë –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê –ê–ë–ó–ê–¶–´:")
        print("-" * 40)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥, —á—Ç–æ –∏ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ
        paragraphs = self._split_into_smart_paragraphs(source_content)

        for i, para in enumerate(paragraphs):
            word_count = len(para.split())
            char_count = len(para)
            print(f"\n  –ê–±–∑–∞—Ü {i + 1}:")
            print(f"    –°–ª–æ–≤: {word_count}, —Å–∏–º–≤–æ–ª–æ–≤: {char_count}")
            print(f"    –¢–µ–∫—Å—Ç: {para[:200]}..." if len(para) > 200 else f"    –¢–µ–∫—Å—Ç: {para}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ—Ö–æ–¥–∏—Ç –ª–∏ —Ñ–∏–ª—å—Ç—Ä
            if word_count < 10 or char_count < 50:
                print(f"    ‚ö†Ô∏è –ù–ï –ü–†–û–•–û–î–ò–¢ —Ñ–∏–ª—å—Ç—Ä (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π)")
            else:
                print(f"    ‚úÖ –ü–†–û–•–û–î–ò–¢ —Ñ–∏–ª—å—Ç—Ä")

        return paragraphs

    def debug_citation_comparison(self, citation_text: str, paragraphs: List[str]):
        """
        –ü–æ–¥—Ä–æ–±–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç—ã —Å –∫–∞–∂–¥—ã–º –∞–±–∑–∞—Ü–µ–º
        """
        print("\n" + "=" * 80)
        print("üîç –î–ï–¢–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –¶–ò–¢–ê–¢–´ –° –ê–ë–ó–ê–¶–ê–ú–ò")
        print("=" * 80)

        print(f"\nüìù –¶–ò–¢–ê–¢–ê:")
        print(f"  {citation_text[:200]}...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Ü–∏—Ç–∞—Ç—ã
        key_phrases = self.extract_key_phrases(citation_text)
        print(f"\nüîë –ö–õ–Æ–ß–ï–í–´–ï –§–†–ê–ó–´ –ò–ó –¶–ò–¢–ê–¢–´:")
        for i, phrase in enumerate(key_phrases[:10]):
            print(f"  {i + 1}. '{phrase}'")

        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –° –ö–ê–ñ–î–´–ú –ê–ë–ó–ê–¶–ï–ú:")
        print("-" * 80)

        for i, para in enumerate(paragraphs):
            word_count = len(para.split())
            char_count = len(para)

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
            if word_count < 10 or char_count < 50:
                continue

            print(f"\nüìë –ê–ë–ó–ê–¶ {i + 1} ({word_count} —Å–ª–æ–≤, {char_count} —Å–∏–º–≤–æ–ª–æ–≤):")

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = self.calculate_semantic_similarity(citation_text, para)
            print(f"  üìä –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
            para_lower = para.lower()
            found_phrases = []

            for phrase in key_phrases:
                phrase_lower = phrase.lower()
                if phrase_lower in para_lower:
                    found_phrases.append(phrase)
                    print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ñ—Ä–∞–∑–∞: '{phrase}'")

            if not found_phrases:
                print(f"  ‚ùå –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            print(f"  üìñ –¢–µ–∫—Å—Ç –∞–±–∑–∞—Ü–∞:")
            print(f"    {para[:300]}...")
            print(f"  {'=' * 40}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
semantic_matcher = FixedSemanticCitationMatcher(language='russian')