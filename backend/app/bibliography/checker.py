from typing import List, Dict, Any, Optional
from app.models.data_models import TextBlock, BibliographyEntry
import re
import requests
import time
from urllib.parse import quote
import json
from app.search.online_searcher import OnlineSearcher, SearchResult
from app.config import APIConfig
from app.search.russian_sources import RussianSourcesSearcher
from app.services.library_service import library_service

class BibliographyChecker:
    def __init__(self):
        self.biblio_keywords = [
            '—Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', '—Å–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã', '–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è',
            '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–∏—Å—Ç–æ—á–Ω–∏–∫–∏', 'references', 'bibliography',
            'reference', 'source', 'works cited', 'literature'
        ]
        self.section_end_keywords = ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', 'appendix', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', 'conclusion']

        self.search_apis = {
            'google_books': 'https://www.googleapis.com/books/v1/volumes',
            'crossref': 'https://api.crossref.org/works',
            'open_library': 'https://openlibrary.org/search.json',
            'semantic_scholar': 'https://api.semanticscholar.org/graph/v1/paper/search'
        }
        self.searcher = OnlineSearcher(APIConfig())
        self.russian_searcher = RussianSourcesSearcher()
        self.library_service = library_service

    def find_bibliography_section(self, text_blocks: List[TextBlock]) -> List[TextBlock]:
        print("–ü–æ–∏—Å–∫ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏...")
        bibliography_blocks = []
        in_bibliography = False
        found_header = False
        non_biblio_count = 0

        for block in text_blocks:
            text = block.text.strip()
            text_lower = text.lower()

            if (not found_header and
                    any(keyword in text_lower for keyword in self.biblio_keywords) and
                    '...' not in text and
                    len(text) < 100):
                print(f"–ù–∞–π–¥–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏: '{text}'")
                in_bibliography = True
                found_header = True
                continue

            if in_bibliography:
                if self._is_bibliography_entry(text):
                    bibliography_blocks.append(block)
                    non_biblio_count = 0
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å: {text[:60]}...")
                else:
                    non_biblio_count += 1
                    if non_biblio_count >= 3:
                        print(f"‚Ñπ –û–±–Ω–∞—Ä—É–∂–µ–Ω –∫–æ–Ω–µ—Ü –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏ (–ø–æ–¥—Ä—è–¥ {non_biblio_count} –Ω–µ-–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤)")
                        break
                    if self._is_definitely_not_bibliography(text):
                        print(f"‚Ñπ –û–±–Ω–∞—Ä—É–∂–µ–Ω —è–≤–Ω–æ –Ω–µ-–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –±–ª–æ–∫: {text[:50]}...")
                        break
                    if self._looks_like_table_data(text):
                        print(f"‚Ñπ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã: {text[:50]}...")
                        break

        print(f"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏: {len(bibliography_blocks)}")
        return bibliography_blocks

    def enhance_with_online_search(self, bibliography_entries: List[BibliographyEntry]) -> List[BibliographyEntry]:
        """–£–ª—É—á—à–∞–µ—Ç –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏ —Å –ø–æ–º–æ—â—å—é –æ–Ω–ª–∞–π–Ω-–ø–æ–∏—Å–∫–∞"""
        print(f"\nüîç –£–õ–£–ß–®–ê–ï–ú –ë–ò–ë–õ–ò–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ï –ó–ê–ü–ò–°–ò ({len(bibliography_entries)} –∑–∞–ø–∏—Å–µ–π)")
        print("=" * 80)

        enhanced_entries = []

        for i, entry in enumerate(bibliography_entries):
            print(f"\nüìñ –ó–ê–ü–ò–°–¨ {i + 1}/{len(bibliography_entries)}: '{entry.text[:80]}...'")
            print("-" * 80)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            search_queries = self._generate_search_queries(entry.text)
            entry.search_queries = search_queries
            print(f"üìã –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {search_queries}")

            best_result = None

            # 1. –°–ù–ê–ß–ê–õ–ê –ò–©–ï–ú –í –õ–û–ö–ê–õ–¨–ù–û–ô –ë–ò–ë–õ–ò–û–¢–ï–ö–ï (–í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
            print(f"\nüîé –®–ê–ì 1: –ü–û–ò–°–ö –í –õ–û–ö–ê–õ–¨–ù–û–ô –ë–ò–ë–õ–ò–û–¢–ï–ö–ï")
            library_match = self._search_in_library(entry.text, search_queries)

            if library_match:
                print(f"‚úÖ –ù–ê–ô–î–ï–ù–û –í –õ–û–ö–ê–õ–¨–ù–û–ô –ë–ò–ë–õ–ò–û–¢–ï–ö–ï!")
                best_result = self._convert_library_match_to_search_result(library_match)

                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
                entry.library_match = {
                    'source_id': library_match.get('id'),
                    'title': library_match.get('title'),
                    'authors': library_match.get('authors', []),
                    'year': library_match.get('year'),
                    'publisher': library_match.get('publisher'),
                    'journal': library_match.get('journal'),
                    'has_file': library_match.get('has_file', False),
                    'has_content': library_match.get('has_content', False),
                    'match_score': library_match.get('match_score', 0),
                    'matched_fields': library_match.get('matched_fields', []),
                    'matched_at': time.time()
                }

                print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_result.confidence:.2f})")
                print(f"üè∑Ô∏è  –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {library_match.get('match_score')}%")
            else:
                # 2. –ï–°–õ–ò –ù–ï –ù–ê–®–õ–ò –í –ë–ò–ë–õ–ò–û–¢–ï–ö–ï - –ò–©–ï–ú –û–ù–õ–ê–ô–ù
                print(f"\nüîé –®–ê–ì 2: –ü–û–ò–°–ö –í –û–ù–õ–ê–ô–ù –ò–°–¢–û–ß–ù–ò–ö–ê–• (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)")

                # —Å–Ω–∞—á–∞–ª–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                print(f"üåê –ü—Ä–æ–±—É–µ–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                russian_result = self.russian_searcher.search_publication(
                    search_queries[0] if search_queries else entry.text,
                    entry.text
                )

                if russian_result:
                    best_result = self._convert_russian_result_to_search_result(russian_result)
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_result.confidence:.2f})")
                else:
                    # –µ—Å–ª–∏ –Ω–µ —Ä–æ—Å —Ç–æ –º–µ–∂–¥—É–Ω–∞—Ä
                    print(f"üåç –ü—Ä–æ–±—É–µ–º –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                    for query in search_queries:
                        print(f"   üîé –ü–æ–∏—Å–∫: '{query}'")
                        results = self.searcher.search_publication(query)

                        if results:
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            relevant_results = [r for r in results if self._is_relevant_result(r, entry.text)]
                            if relevant_results:
                                best_result = self._filter_best_result(relevant_results, entry.text)
                                if best_result:
                                    print(f"   ‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_result.confidence:.2f})")
                                    break
                            else:
                                print(f"   ‚ö† –ù–∞–π–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ")
                        else:
                            print(f"   ‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç")

            if best_result:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º SearchResult –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è online_metadata
                entry.online_metadata = {
                    'source': best_result.source,
                    'title': best_result.title,
                    'authors': best_result.authors or [],
                    'year': best_result.year,
                    'publisher': best_result.publisher,
                    'journal': best_result.journal,
                    'volume': best_result.volume,
                    'issue': best_result.issue,
                    'pages': best_result.pages,
                    'doi': best_result.doi,
                    'isbn': best_result.isbn,
                    'url': best_result.url,
                    'confidence': best_result.confidence,
                    'retrieved_at': time.time(),
                }
                entry.enhancement_confidence = best_result.confidence
                entry.is_verified = True

                source_name = "–±–∏–±–ª–∏–æ—Ç–µ–∫–µ" if best_result.source == 'personal_library' else best_result.source
                print(f"‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢ –ò–ó {source_name.upper()} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_result.confidence:.2f})")
            else:
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ online_metadata —ç—Ç–æ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å, –∞ –Ω–µ None
                entry.online_metadata = {}
                print(f"‚ùå –ü–û–î–•–û–î–Ø–©–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢ –ù–ï –ù–ê–ô–î–ï–ù")

            enhanced_entries.append(entry)

        print(f"\n{'=' * 80}")
        print(
            f"üìä –ò–¢–û–ì: –£–ª—É—á—à–µ–Ω–æ {len([e for e in enhanced_entries if e.online_metadata])} –∏–∑ {len(enhanced_entries)} –∑–∞–ø–∏—Å–µ–π")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
        library_matches = [e for e in enhanced_entries if e.library_match]
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ: {len(library_matches)} –∑–∞–ø–∏—Å–µ–π")

        return enhanced_entries

    def _convert_library_match_to_search_result(self, library_match: Dict) -> SearchResult:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≤ SearchResult"""
        return SearchResult(
            source='personal_library',
            title=library_match.get('title', ''),
            authors=library_match.get('authors', []),
            year=library_match.get('year'),
            publisher=library_match.get('publisher'),
            journal=library_match.get('journal'),
            volume=None,
            issue=None,
            pages=None,
            doi=library_match.get('doi'),
            isbn=library_match.get('isbn'),
            url=library_match.get('url'),
            confidence=min(library_match.get('match_score', 60) / 100.0, 1.0),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º score –≤ confidence 0-1
            is_search_link=False
        )

    def _search_in_library(self, entry_text: str, search_queries: List[str]) -> Optional[Dict[str, Any]]:
        """–ò—â–µ—Ç –∑–∞–ø–∏—Å—å –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        try:
            print(f"\n      üîç –ü–û–ò–°–ö –í –õ–û–ö–ê–õ–¨–ù–û–ô –ë–ò–ë–õ–ò–û–¢–ï–ö–ï –î–õ–Ø: '{entry_text[:80]}...'")
            original_text = entry_text
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø–∏—Å–∏
            search_params = self._extract_search_params_from_entry(entry_text)
            print(f"      üìä –ü–ê–†–ê–ú–ï–¢–†–´ –ü–û–ò–°–ö–ê: {search_params}")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º user_id –¥–ª—è –¥–µ–º–æ (–≤ production —ç—Ç–æ –±—É–¥–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π user_id)
            user_id = "demo_user"
            print(f"      üë§ USER ID: {user_id}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å library_service
            if not hasattr(self, 'library_service') or self.library_service is None:
                print(f"      ‚ùå library_service –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!")
                return None

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if not hasattr(self.library_service, 'sources'):
                print(f"      ‚ùå library_service.sources –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!")
                return None

            user_sources = self.library_service.sources.get(user_id, [])
            print(f"      üìö –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {len(user_sources)}")

            if not user_sources:
                print(f"      üì≠ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—É—Å—Ç–∞")
                return None

            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"      üîé –ü–ï–†–í–´–ï 5 –ò–°–¢–û–ß–ù–ò–ö–û–í –í –ë–ò–ë–õ–ò–û–¢–ï–ö–ï:")
            for i, source in enumerate(user_sources[:5]):
                print(f"        {i + 1}. '{source.get('title', 'No title')}'")
                print(f"           –ê–≤—Ç–æ—Ä—ã: {source.get('authors', [])}")
                print(f"           –ì–æ–¥: {source.get('year')}")
                if source.get('doi'):
                    print(f"           DOI: {source.get('doi')}")

            best_match = None
            best_score = 0
            all_matches = []
            used_source_ids = set()

            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            for source in user_sources:
                if source.get('id') in used_source_ids:
                    continue

                score = self._calculate_library_match_score(source, search_params)

                if score > 0:
                    all_matches.append({
                        'source': source,
                        'score': score,
                        'matched_fields': self._get_matched_fields(source, search_params)
                    })

                    if score > best_score:
                        best_score = score
                        best_match = source
                        print(f"      üéØ –ù–û–í–û–ï –õ–£–ß–®–ï–ï –°–û–í–ü–ê–î–ï–ù–ò–ï: {score} –±–∞–ª–ª–æ–≤")
                        print(f"        –ù–∞–∑–≤–∞–Ω–∏–µ: {source.get('title', 'No title')}")

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
            if best_match and best_score >= 80:
                print(f"      ‚úÖ –ù–ê–ô–î–ï–ù–û –°–û–í–ü–ê–î–ï–ù–ò–ï –í –ë–ò–ë–õ–ò–û–¢–ï–ö–ï!")
                print(f"      üìä –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score} –±–∞–ª–ª–æ–≤")
                print(f"      üìñ –ò—Å—Ç–æ—á–Ω–∏–∫: {best_match.get('title', 'No title')}")
                used_source_ids.add(best_match.get('id'))
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result = {
                    'id': best_match.get('id'),
                    'title': best_match.get('title'),
                    'authors': best_match.get('authors', []),
                    'year': best_match.get('year'),
                    'publisher': best_match.get('publisher'),
                    'journal': best_match.get('journal'),
                    'doi': best_match.get('doi'),
                    'isbn': best_match.get('isbn'),
                    'url': best_match.get('url'),
                    'has_file': best_match.get('has_file', False),
                    'has_content': best_match.get('has_content', False),
                    'full_content': best_match.get('full_content', ''),
                    'content_preview': best_match.get('content_preview', ''),
                    'text_length': best_match.get('text_length', 0),
                    'match_score': best_score,
                    'matched_fields': self._get_matched_fields(best_match, search_params)
                }

                print(f"      üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.get('title')}")
                print(f"      üéØ –ë–∞–ª–ª—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {best_score}")
                return result
            elif best_match and best_score >= 60:
                print(f"      üìä –•–û–†–û–®–ï–ï –°–û–í–ü–ê–î–ï–ù–ò–ï: {best_score} –±–∞–ª–ª–æ–≤")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ª–æ–∂–Ω—ã–º —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ–º
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ
                if self._check_authors_strict(search_params, best_match):
                    print(f"      ‚úÖ –ê–í–¢–û–†–´ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–´ - –ò–°–ü–û–õ–¨–ó–£–ï–ú")
                    # ... –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç ...
                else:
                    print(f"      ‚ö† –ê–í–¢–û–†–´ –ù–ï –°–û–í–ü–ê–î–ê–Æ–¢ - –ü–†–û–ü–£–°–ö–ê–ï–ú")
                    return None
            else:
                print(f"      ‚ùå –ù–ï–¢ –î–û–°–¢–ê–¢–û–ß–ù–û –•–û–†–û–®–ò–• –°–û–í–ü–ê–î–ï–ù–ò–ô –í –ë–ò–ë–õ–ò–û–¢–ï–ö–ï")
                print(f"      üìä –õ—É—á—à–∏–π score: {best_score} (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 60)")
                if all_matches:
                    print(f"      üìà –í—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è:")
                    for match in sorted(all_matches, key=lambda x: x['score'], reverse=True)[:3]:
                        print(f"        - {match['score']} –±–∞–ª–ª–æ–≤: {match['source'].get('title')}")
                return None

        except Exception as e:
            print(f"      ‚ùå –û–®–ò–ë–ö–ê –ü–†–ò –ü–û–ò–°–ö–ï –í –ë–ò–ë–õ–ò–û–¢–ï–ö–ï: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _get_matched_fields(self, source: Dict, search_params: Dict) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"""
        matched_fields = []

        if search_params.get('doi') and source.get('doi'):
            if search_params['doi'].lower() == source['doi'].lower():
                matched_fields.append('doi')

        if search_params.get('isbn') and source.get('isbn'):
            if search_params['isbn'].replace('-', '') == source['isbn'].replace('-', ''):
                matched_fields.append('isbn')

        if search_params.get('title') and source.get('title'):
            search_title = search_params['title'].lower()
            source_title = source['title'].lower()
            if search_title == source_title:
                matched_fields.append('title_exact')
            elif search_title in source_title or source_title in search_title:
                matched_fields.append('title_partial')
            else:
                search_words = set(re.findall(r'\w+', search_title))
                source_words = set(re.findall(r'\w+', source_title))
                if search_words.intersection(source_words):
                    matched_fields.append('title_words')

        if search_params.get('authors') and source.get('authors'):
            search_authors = [a.lower().strip() for a in search_params['authors'] if a.strip()]
            source_authors = [a.lower().strip() for a in source['authors'] if a.strip()]

            for search_author in search_authors:
                search_surname = search_author.split()[0] if search_author.split() else search_author
                for source_author in source_authors:
                    source_surname = source_author.split()[0] if source_author.split() else source_author
                    if search_surname == source_surname:
                        matched_fields.append('authors')
                        break

        if search_params.get('year') and source.get('year'):
            if str(search_params['year']) == str(source['year']):
                matched_fields.append('year')

        return list(set(matched_fields))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def _calculate_library_match_score(self, source: Dict, search_params: Dict) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø"""
        score = 0

        print(f"\n        üîç –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º: '{source.get('title', 'No title')[:50]}...'")

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        search_title = (search_params.get('title') or '').lower().strip()
        source_title = (source.get('title') or '').lower().strip()

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ DOI/ISBN (—Å–∞–º—ã–µ —Ç–æ—á–Ω—ã–µ) - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –∏—Ö –Ω–µ—Ç

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê
        if search_title and source_title:
            # –£–±–∏—Ä–∞–µ–º –í–°–ï –∏–Ω–∏—Ü–∏–∞–ª—ã, —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            def clean_text(text):
                # –£–¥–∞–ª—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã —Ç–∏–ø–∞ "–∞.", "—Å.", "–º."
                text = re.sub(r'\b[–∞-—è]\.\s*', '', text)
                # –£–¥–∞–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±—É–∫–≤—ã —Å —Ç–æ—á–∫–∞–º–∏
                text = re.sub(r'\b[–∞-—è—ë]\.', '', text)
                # –£–¥–∞–ª—è–µ–º –∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏, –¥–≤–æ–µ—Ç–æ—á–∏—è
                text = re.sub(r'[.,:;]', '', text)
                # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω—å—à–µ 3 –±—É–∫–≤)
                words = text.split()
                words = [w for w in words if len(w) > 2]
                return ' '.join(words).lower()

            clean_search = clean_text(search_title)
            clean_source = clean_text(source_title)

            print(f"        üîß –û—á–∏—â–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏:")
            print(f"           –ò—â–µ–º: '{clean_search}'")
            print(f"           –í: '{clean_source}'")

            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
            if clean_search == clean_source:
                score += 70
                print(f"        ‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –ù–ê–ó–í–ê–ù–ò–Ø (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏) (+70)")

            # –û–¥–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥–æ–µ
            elif clean_search in clean_source or clean_source in clean_search:
                score += 60
                print(f"        ‚úÖ –ß–ê–°–¢–ò–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –ù–ê–ó–í–ê–ù–ò–Ø (+60)")

            # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã—Ö!)
            else:
                search_words = set(clean_search.split())
                source_words = set(clean_source.split())
                common_words = search_words.intersection(source_words)

                # –§–∏–ª—å—Ç—Ä—É–µ–º: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π > 4 (–∑–Ω–∞—á–∏–º—ã–µ)
                significant_common = {w for w in common_words if len(w) > 4}

                if significant_common:
                    keyword_score = len(significant_common) * 20
                    score += keyword_score
                    print(f"        ‚úÖ –û–ë–©–ò–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {significant_common} (+{keyword_score})")
                else:
                    # –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–∏–Ω–∏—Ü–∏–∞–ª—ã –∏ —Ç.–¥.) - –ù–ï –î–ê–ï–ú –ë–ê–õ–õ–û–í!
                    print(f"        ‚ö† –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {common_words} (0 –±–∞–ª–ª–æ–≤)")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–æ–≤ - –°–ê–ú–û–ï –í–ê–ñ–ù–û–ï!
        if search_params.get('authors') and source.get('authors'):
            search_authors = [a.lower().strip() for a in search_params['authors'] if a and len(a) > 2]
            source_authors = [a.lower().strip() for a in source['authors'] if a and len(a) > 2]

            print(f"        üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤:")
            print(f"          –ò—â–µ–º: {search_authors}")
            print(f"          –í: {source_authors}")

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            search_authors = list(set(search_authors))
            source_authors = list(set(source_authors))

            author_matches = 0
            for search_author in search_authors:
                for source_author in source_authors:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —É–±–∏—Ä–∞–µ–º —Ç–æ—á–∫–∏, –∏–Ω–∏—Ü–∏–∞–ª—ã
                    norm_search = self._normalize_author_name(search_author)
                    norm_source = self._normalize_author_name(source_author)

                    if norm_search and norm_source and norm_search == norm_source:
                        author_matches += 1
                        print(f"        ‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –ê–í–¢–û–†–ê: {search_author} == {source_author}")
                        break

            # –í–ï–° –∞–≤—Ç–æ—Ä–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –í–´–®–ï, —á–µ–º –≤–µ—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞!
            if author_matches > 0:
                score += author_matches * 50  # 50 –±–∞–ª–ª–æ–≤ –∑–∞ –∫–∞–∂–¥–æ–≥–æ —Å–æ–≤–ø–∞–≤—à–µ–≥–æ –∞–≤—Ç–æ—Ä–∞
                print(f"        üìä –ê–≤—Ç–æ—Ä—Å–∫–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {author_matches} (+{author_matches * 50} –±–∞–ª–ª–æ–≤)")
            else:
                # –ï—Å–ª–∏ –∞–≤—Ç–æ—Ä—ã –ù–ï —Å–æ–≤–ø–∞–¥–∞—é—Ç - –°–ò–õ–¨–ù–´–ô –®–¢–†–ê–§
                score -= 40
                print(f"        ‚ùå –ê–í–¢–û–†–´ –ù–ï –°–û–í–ü–ê–î–ê–Æ–¢! (-40)")

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ–¥–∞
        if search_params.get('year') and source.get('year'):
            search_year = str(search_params['year']).strip()
            source_year = str(source['year']).strip()
            if search_year == source_year:
                score += 20
                print(f"        ‚úÖ –°–û–í–ü–ê–î–ï–ù–ò–ï –ì–û–î–ê: {search_year} (+20)")
            else:
                score -= 15
                print(f"        ‚ùå –ù–ï–°–û–í–ü–ê–î–ï–ù–ò–ï –ì–û–î–ê: {search_year} != {source_year} (-15)")

        print(f"        üìä –ò–¢–û–ì–û–í–´–ô SCORE: {score} –±–∞–ª–ª–æ–≤")
        return max(score, 0)  # –ù–µ –º–µ–Ω—å—à–µ 0

    def _extract_search_params_from_entry(self, entry_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏ - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        clean_text = re.sub(r'\s+', ' ', entry_text.strip())
        print(f"\n        üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: '{clean_text}'")

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥)
        full_title = self._extract_complete_title(clean_text)

        # 2. –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
        if not full_title:
            full_title = self._extract_title(clean_text)

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä—ã
        authors = self._extract_authors(clean_text)

        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥
        year = self._extract_year(clean_text)

        # 5. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        doi = None
        isbn = None
        publisher = None
        journal = None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º DOI
        doi_patterns = [
            r'doi:\s*([^\s,.;]+)',
            r'DOI:\s*([^\s,.;]+)',
            r'https?://doi\.org/([^\s]+)',
            r'\b10\.\d{4,9}/[^\s]+'
        ]
        for pattern in doi_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                doi = match.group(1).strip()
                break

        # –ò–∑–≤–ª–µ–∫–∞–µ–º ISBN
        isbn_patterns = [
            r'ISBN[\s:-]*([\d\-X]{10,17})',
            r'ISBN\s+([\d\-X]{10,17})',
            r'\b[\d\-X]{10,17}\b(?=.*ISBN)',
        ]
        for pattern in isbn_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                isbn = match.group(1).strip()
                break

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        publisher_patterns = [
            r'‚Äî\s*[^:]*:\s*([^.,;]+?)(?=\.|,|;|\s*\d|$)',
            r':\s*([^.,;]+?)(?=\.|,|;|\s*\d|$)',
            r'–∏–∑–¥-–≤–æ\s+([^.,;]+)',
            r'–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ\s+([^.,;]+)',
        ]
        for pattern in publisher_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                publisher = match.group(1).strip()
                break

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∂—É—Ä–Ω–∞–ª
        journal_patterns = [
            r'//\s*([^.,]+?)(?=\.|,|\s*\d|$)',
            r'–∂—É—Ä–Ω–∞–ª\s+([^.,;]+)',
            r'–ñ—É—Ä–Ω–∞–ª\s+([^.,;]+)',
        ]
        for pattern in journal_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                journal = match.group(1).strip()
                break

        result = {
            'title': full_title,
            'authors': authors,
            'year': year,
            'doi': doi,
            'isbn': isbn,
            'publisher': publisher,
            'journal': journal,
            'original_text': clean_text
        }

        print(f"        üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞:")
        print(f"          üìñ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {full_title}")
        print(f"          üë• –ê–≤—Ç–æ—Ä—ã: {authors}")
        print(f"          üìÖ –ì–æ–¥: {year}")
        print(f"          üîó DOI: {doi}")
        print(f"          üìò ISBN: {isbn}")
        print(f"          üè¢ –ò–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {publisher}")
        print(f"          üì∞ –ñ—É—Ä–Ω–∞–ª: {journal}")

        return result

    def _extract_complete_title(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏"""
        if not text:
            return None

        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        text = re.sub(r'\s+', ' ', text.strip())

        print(f"        üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑: '{text[:100]}...'")

        # 1. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ–∂–¥—É –∞–≤—Ç–æ—Ä–∞–º–∏ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        # –ü–∞—Ç—Ç–µ—Ä–Ω: –∞–≤—Ç–æ—Ä—ã [–Ω–∞–∑–≤–∞–Ω–∏–µ] : —Ç–∏–ø / —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã –∏ —Ç.–¥.

        # –£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ (–≤—Å–µ –¥–æ –ø–µ—Ä–≤–æ–π —Ç–æ—á–∫–∏, –¥–≤–æ–µ—Ç–æ—á–∏—è –∏–ª–∏ –≥–æ–¥–∞)
        text_without_authors = text

        # –ò—â–µ–º –∫–æ–Ω–µ—Ü –∞–≤—Ç–æ—Ä—Å–∫–æ–≥–æ –±–ª–æ–∫–∞
        author_end_patterns = [
            r'^[^.]*\.\s*',  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ç–æ—á–∫–æ–π
            r'^[^:]*:\s*',  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
            r'^[^/]*/\s*',  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Å–ª–µ—à–µ–º
        ]

        for pattern in author_end_patterns:
            match = re.match(pattern, text)
            if match:
                text_without_authors = text[len(match.group(0)):].strip()
                break

        # 2. –¢–µ–ø–µ—Ä—å –∏—â–µ–º –∫–æ–Ω–µ—Ü –Ω–∞–∑–≤–∞–Ω–∏—è
        # –ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥:
        # - ": —É—á–µ–±–Ω–∏–∫", ": –ø–æ—Å–æ–±–∏–µ" –∏ —Ç.–¥.
        # - " / " (—Ä–µ–¥–∞–∫—Ç–æ—Ä—ã)
        # - ". ‚Äî " (–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ)
        # - ", " (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è)

        title_end_patterns = [
            r'^([^:]+?)(?=:\s*(?:—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è|—É—á–µ–±–Ω–æ–µ\s+–ø–æ—Å–æ–±–∏–µ|—É—á–µ–±–Ω–æ-–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–µ))',
            r'^([^/]+?)(?=/\s*[–ê-–Ø–ÅA-Z])',  # –ü–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞–º–∏
            r'^([^.]+?)(?=\.\s*‚Äî)',  # –ü–µ—Ä–µ–¥ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º
            r'^([^,]+?)(?=,\s*\d{4})',  # –ü–µ—Ä–µ–¥ –≥–æ–¥–æ–º
            r'^([^;]+)',  # –î–æ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π
            r'^([^.]+)',  # –î–æ —Ç–æ—á–∫–∏
        ]

        for pattern in title_end_patterns:
            match = re.search(pattern, text_without_authors)
            if match:
                title = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                title = re.sub(r'^[.,:;\s]+', '', title)
                title = re.sub(r'[.,:;\s]+$', '', title)

                if title and len(title) > 5:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                    if (len(title) >= 10 and
                            not any(word in title.lower() for word in ['—Ç.', '–≤—ã–ø.', '—Å.', '–≥.', '–∏–∑–¥-–≤–æ']) and
                            re.search(r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]', title)):
                        print(f"        ‚úÖ –ù–∞–π–¥–µ–Ω –ø–æ–ª–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{title}'")
                        return title

        # 3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        words = text_without_authors.split()
        meaningful_words = []

        # –ò—â–µ–º –ø–µ—Ä–≤—ã–µ 5-10 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
        for word in words[:15]:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
            if (len(word) > 2 and
                    not word.lower() in ['–ø–æ–¥', '—Ä–µ–¥', '—Ä–µ–¥.', '–∏–∑–¥-–≤–æ', '–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ'] and
                    not re.match(r'^[A-Z]\.$', word) and  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã
                    not re.match(r'^\d+$', word)):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å–ª–∞
                meaningful_words.append(word)

            if len(meaningful_words) >= 8:
                break

        if meaningful_words:
            title = ' '.join(meaningful_words)
            # –û—á–∏—â–∞–µ–º –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            title = re.sub(r'[.,:;‚Äî/]$', '', title).strip()

            if len(title) > 10:
                print(f"        üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–≤: '{title}'")
                return title

        print(f"        ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–æ–ª–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
        return None

    def _convert_russian_result_to_search_result(self, russian_result: Dict[str, Any]) -> SearchResult:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ SearchResult"""
        url = russian_result.get('record_url') or russian_result.get('url')

        return SearchResult(
            source=russian_result['source'],
            title=russian_result.get('title', ''),
            authors=russian_result.get('authors', []),
            year=russian_result.get('year'),
            publisher=russian_result.get('publisher'),
            journal=russian_result.get('journal'),
            volume=None,
            issue=None,
            pages=None,
            doi=None,
            isbn=None,
            url=url,
            confidence=russian_result.get('confidence', 0.6),
            is_search_link=russian_result.get('is_search_link', False)
        )

    def _enhance_single_entry(self, entry: BibliographyEntry) -> BibliographyEntry:
        """–£–ª—É—á—à–∞–µ—Ç –æ–¥–Ω—É –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –∑–∞–ø–∏—Å—å"""
        search_queries = self._generate_search_queries(entry.text)
        entry.search_queries = search_queries

        best_overall_result = None
        best_confidence = 0.0

        for query in search_queries:
            print(f"      –ü–æ–∏—Å–∫: '{query}'")
            try:
                results = self.online_searcher.search_publication(query)

                if results:
                    best_result = self._filter_best_result(results, query)

                    if best_result and best_result.confidence > best_confidence:
                        best_overall_result = best_result
                        best_confidence = best_result.confidence
                        print(f"      –ù–∞–π–¥–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_result.confidence:.2f})")

                        if best_result.confidence > 0.8:
                            break
                else:
                    print(f"      –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è: {query}")

            except Exception as e:
                print(f"      –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
                continue

        if best_overall_result and best_confidence > 0.3:
            entry.online_metadata = self._format_online_metadata(best_overall_result)
            entry.is_verified = True
            entry.enhancement_confidence = best_confidence
            print(f"      –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é: {best_confidence:.2f}")
        else:
            print(f"      –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        return entry

    def _format_online_metadata(self, result: SearchResult) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        return {
            'source': result.source,
            'title': result.title,
            'authors': result.authors,
            'year': result.year,
            'publisher': result.publisher,
            'journal': result.journal,
            'volume': result.volume,
            'issue': result.issue,
            'pages': result.pages,
            'doi': result.doi,
            'isbn': result.isbn,
            'url': result.url,
            'confidence': result.confidence,
            'retrieved_at': time.time()
        }

    def _generate_search_queries(self, text: str) -> List[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        queries = []

        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        clean_text = re.sub(r'\[.*?\]', '', text)
        clean_text = re.sub(r'[^\w\s.,;:()-]', '', clean_text)

        # 1. –û—Å–Ω–æ–≤–Ω–æ–π –æ—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        if clean_text.strip():
            queries.append(clean_text.strip())

        # 2. –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        simple_text = re.sub(
            r'\b(–∏–∑–¥-–≤–æ|–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ|—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è|—Å—Ç–∞—Ç—å—è|–ø–æ–¥ —Ä–µ–¥|—Ä–µ–¥\.|—Å\.|—Å—Ç—Ä\.|—Ç\.|–≤—ã–ø\.)\b.*?[.,]', '',
            clean_text, flags=re.IGNORECASE)
        simple_text = re.sub(r'\d+\.\d+|\d+-\d+', '', simple_text)  # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
        if simple_text.strip() and simple_text != clean_text:
            queries.append(simple_text.strip())

        # 3. –ó–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ—Ä–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        authors = self._extract_authors(clean_text)
        title = self._extract_title(clean_text)
        if authors and title:
            queries.append(f"{authors} {title}")

        # 4. –ó–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º
        improved_title = self._extract_improved_title(clean_text)
        if improved_title:
            queries.append(improved_title)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        unique_queries = []
        seen = set()
        for query in queries:
            if query and len(query) > 10 and query not in seen:
                seen.add(query)
                unique_queries.append(query)

        return unique_queries[:4]

    def _extract_improved_title(self, text: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""
        # –£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ (–≤—Å—ë –¥–æ –ø–µ—Ä–≤–æ–π —Ç–æ—á–∫–∏ –∏–ª–∏ –¥–≤–æ–µ—Ç–æ—á–∏—è)
        text_without_authors = re.sub(r'^[^.:]*[.:]', '', text).strip()

        # –£–±–∏—Ä–∞–µ–º –≥–æ–¥
        text_without_year = re.sub(r'\b(19|20)\d{2}\b', '', text_without_authors)

        # –£–±–∏—Ä–∞–µ–º –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –ø—Ä–æ—á—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        patterns_to_remove = [
            r'\/\/.*$',  # –í—Å—ë –ø–æ—Å–ª–µ //
            r'‚Äî.*$',  # –í—Å—ë –ø–æ—Å–ª–µ ‚Äî
            r'\.‚Äî.*$',  # –í—Å—ë –ø–æ—Å–ª–µ .‚Äî
            r'\(.*\)',  # –°–∫–æ–±–∫–∏ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
            r'\b(–∏–∑–¥-–≤–æ|–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ|—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è|—Å—Ç–∞—Ç—å—è)\b.*$',
        ]

        for pattern in patterns_to_remove:
            text_without_year = re.sub(pattern, '', text_without_year)

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5-8 —Å–ª–æ–≤ –∫–∞–∫ –≤–æ–∑–º–æ–∂–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        words = text_without_year.strip().split()
        if len(words) > 2:
            return ' '.join(words[:min(8, len(words))])

        return None

    def _extract_search_params_from_entry(self, entry_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        clean_text = re.sub(r'\s+', ' ', entry_text.strip())
        print(f"\n        üìù –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: '{clean_text[:100]}...'")

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = self._extract_title(clean_text)

        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä—ã (—Å–ø–∏—Å–æ–∫ —Ñ–∞–º–∏–ª–∏–π)
        authors = self._extract_authors_list(clean_text)

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ–¥
        year = self._extract_year(clean_text)

        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º DOI
        doi = None
        doi_patterns = [
            r'doi:\s*([^\s,.;]+)',
            r'DOI:\s*([^\s,.;]+)',
            r'https?://doi\.org/([^\s]+)',
            r'\b10\.\d{4,9}/[^\s]+'
        ]
        for pattern in doi_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                doi = match.group(1).strip()
                break

        # 5. –ò–∑–≤–ª–µ–∫–∞–µ–º ISBN
        isbn = None
        isbn_patterns = [
            r'ISBN[\s:-]*([\d\-X]{10,17})',
            r'ISBN\s+([\d\-X]{10,17})',
            r'\b[\d\-X]{10,17}\b(?=.*ISBN)',
        ]
        for pattern in isbn_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                isbn = match.group(1).strip()
                break

        # 6. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        publisher = None
        publisher_patterns = [
            r'‚Äî\s*[^:]*:\s*([^.,;]+?)(?=\.|,|;|\s*\d|$)',
            r':\s*([^.,;]+?)(?=\.|,|;|\s*\d|$)',
            r'–∏–∑–¥-–≤–æ\s+([^.,;]+)',
            r'–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ\s+([^.,;]+)',
        ]
        for pattern in publisher_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                publisher = match.group(1).strip()
                break

        # 7. –ò–∑–≤–ª–µ–∫–∞–µ–º –∂—É—Ä–Ω–∞–ª
        journal = None
        journal_patterns = [
            r'//\s*([^.,]+?)(?=\.|,|\s*\d|$)',
            r'–∂—É—Ä–Ω–∞–ª\s+([^.,;]+)',
            r'–ñ—É—Ä–Ω–∞–ª\s+([^.,;]+)',
        ]
        for pattern in journal_patterns:
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                journal = match.group(1).strip()
                break

        result = {
            'title': title,
            'authors': authors,  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Ñ–∞–º–∏–ª–∏–π
            'year': year,
            'doi': doi,
            'isbn': isbn,
            'publisher': publisher,
            'journal': journal,
            'original_text': clean_text
        }

        print(f"        üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞:")
        print(f"          üìñ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
        print(f"          üë• –ê–≤—Ç–æ—Ä—ã: {authors}")
        print(f"          üìÖ –ì–æ–¥: {year}")
        print(f"          üîó DOI: {doi}")
        print(f"          üìò ISBN: {isbn}")
        print(f"          üè¢ –ò–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {publisher}")
        print(f"          üì∞ –ñ—É—Ä–Ω–∞–ª: {journal}")

        return result

    def _normalize_author_name(self, author: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è –∞–≤—Ç–æ—Ä–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not author:
            return ""

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        author = author.lower().strip()

        # –£–¥–∞–ª—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã –∏ —Ç–æ—á–∫–∏
        author = re.sub(r'[–∞-—è]\.\s*', '', author)  # —Ä—É—Å—Å–∫–∏–µ –∏–Ω–∏—Ü–∏–∞–ª—ã
        author = re.sub(r'[a-z]\.\s*', '', author)  # –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–Ω–∏—Ü–∏–∞–ª—ã
        author = re.sub(r'\.', '', author)  # –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ—á–∫–∏

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        author = re.sub(r'\s+', ' ', author).strip()

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—é (–ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ)
        parts = author.split()
        if parts:
            return parts[0]

        return author

    def _extract_authors_list(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–º–∏–ª–∏–π –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏.
        –†–ê–ë–û–ß–ê–Ø –í–ï–†–°–ò–Ø.
        """
        authors = []

        print(f"        üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–≤—Ç–æ—Ä–æ–≤: '{text[:100]}...'")

        # 1. –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω: –§–∞–º–∏–ª–∏—è, –ò. –û. (—Ä—É—Å—Å–∫–∏–µ –∞–≤—Ç–æ—Ä—ã)
        # –ü—Ä–∏–º–µ—Ä: "–õ–æ–ø–∞—Ä–µ–≤–∞, –ê. –ú." –∏–ª–∏ "–ì—Ä–∞—á–µ–≤, –°. –ê., –ì—É–Ω–¥–æ—Ä–æ–≤–∞, –ú. –ê."
        pattern_russian = r'([–ê-–Ø–Å][–∞-—è—ë]+),\s*[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.'

        matches = re.findall(pattern_russian, text)
        if matches:
            print(f"        ‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–≤—Ç–æ—Ä—ã (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä—É—Å—Å–∫–∏–π): {matches}")
            return matches  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–º–∏–ª–∏–π

        # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –¥—Ä—É–≥–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: –§–∞–º–∏–ª–∏—è –ò.–û.
        pattern_russian2 = r'([–ê-–Ø–Å][–∞-—è—ë]+)\s+[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.'
        matches = re.findall(pattern_russian2, text)
        if matches:
            print(f"        ‚úÖ –ù–∞–π–¥–µ–Ω—ã –∞–≤—Ç–æ—Ä—ã (–ø–∞—Ç—Ç–µ—Ä–Ω —Ä—É—Å—Å–∫–∏–π2): {matches}")
            return matches

        # 3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —Ñ–∞–º–∏–ª–∏–∏ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5-7 —Å–ª–æ–≤ –∫–∞–∫ –≤–æ–∑–º–æ–∂–Ω—ã–π –±–ª–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤
        words = text.split()
        potential_authors = []

        for i, word in enumerate(words[:7]):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ —Å–ª–æ–≤–æ –Ω–∞ —Ñ–∞–º–∏–ª–∏—é
            if (re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+$', word) and
                    len(word) > 2 and
                    word.lower() not in ['–∏–∑–¥', '–ø–æ–¥', '—Ä–µ–¥', '–∞–≤—Ç', '—Å–æ—Å—Ç']):
                potential_authors.append(word)

        if potential_authors:
            print(f"        üë§ –ê–≤—Ç–æ—Ä—ã (fallback): {potential_authors}")
            return potential_authors

        print(f"        ‚ö† –ê–≤—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return []

    def _extract_authors(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤: "–ò–≤–∞–Ω–æ–≤ –ò.–ò.", "–ü–µ—Ç—Ä–æ–≤ –ê.–í."
        patterns = [
            r'^([–ê-–Ø][–∞-—è]+ [–ê-–Ø]\.[–ê-–Ø]\.)',  # –ò–≤–∞–Ω–æ–≤ –ò.–ò.
            r'^([–ê-–Ø][–∞-—è]+ [–ê-–Ø][–∞-—è]+ [–ê-–Ø]\.[–ê-–Ø]\.)',  # –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò.–ò.
            r'^([–ê-–Ø][–∞-—è]+,\s*[–ê-–Ø]\.[–ê-–Ø]\.)',  # –ò–≤–∞–Ω–æ–≤, –ò.–ò.
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤
        patterns_en = [
            r'^([A-Z][a-z]+ [A-Z]\.)',  # Smith J.
            r'^([A-Z][a-z]+ [A-Z]\. [A-Z]\.)',  # Smith J. K.
            r'^([A-Z][a-z]+,\s*[A-Z]\.)',  # Smith, J.
        ]

        for pattern in patterns_en:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _extract_year(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        match = re.search(r'\b(19|20)\d{2}\b', text)
        return match.group(0) if match else None

    def _extract_title(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø"""
        if not text:
            return None

        # 1. –£–¥–∞–ª—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –≤ –Ω–∞—á–∞–ª–µ (–≤—Å—ë –¥–æ –ø–µ—Ä–≤–æ–≥–æ –¥–≤–æ–µ—Ç–æ—á–∏—è –∏–ª–∏ —Ç–æ—á–∫–∏ –ø–æ—Å–ª–µ –∞–≤—Ç–æ—Ä–æ–≤)
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: —Ñ–∞–º–∏–ª–∏—è, –∏–Ω–∏—Ü–∏–∞–ª—ã
        text_without_authors = re.sub(
            r'^[–ê-–Ø–Å][–∞-—è—ë]+(?:,\s*[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.)?(?:\s+–∏\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:,\s*[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.)?)*', '', text)

        # 2. –£–¥–∞–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text_without_authors = text_without_authors.lstrip('.,: ')

        # 3. –£–¥–∞–ª—è–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏
        text_without_authors = re.sub(r'\[.*?\]', '', text_without_authors)

        # 4. –ò—â–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ–±—ã—á–Ω–æ –¥–æ: ":", "/", " ‚Äî ", "("
        patterns = [
            r'^([^:/‚Äî(]+?)(?=:\s*(?:—É—á–µ–±–Ω–∏–∫|–ø–æ—Å–æ–±–∏–µ|—É—á–µ–±–Ω–æ–µ|–ø—Ä–∞–∫—Ç–∏–∫—É–º))',
            r'^([^:/‚Äî(]+?)(?=/\s*[–ê-–Ø–ÅA-Z])',
            r'^([^:/‚Äî(]+?)(?=‚Äî)',
            r'^([^:/‚Äî(]+?)(?=\()',
        ]

        for pattern in patterns:
            match = re.search(pattern, text_without_authors)
            if match:
                title = match.group(1).strip()
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ
                title = re.sub(r'[.,:;‚Äî/]$', '', title).strip()

                if title and len(title) > 3:
                    # –£–¥–∞–ª—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    title = re.sub(r'^[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s*', '', title)
                    return title

        return None

    def _is_definitely_not_bibliography(self, text: str) -> bool:
        text_lower = text.lower()
        not_biblio_indicators = [
            any(word in text_lower for word in ['—Ç.—Ä.', '—Ç—ã—Å. —Ä—É–±.', '—Ä—É–±.', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '–∑–∞–∫—É–ø–∫–∞']),
            re.search(r'\d+\s*—Ç\.—Ä\.', text),
            re.search(r'\d+\s*—Ä—É–±', text),
            any(term in text_lower for term in ['–Ω–¥—Å', '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤']),
            len(text) < 30 and any(char.isdigit() for char in text),
            any(char in text for char in ['+', '-', '*', '/', '=']),
        ]
        return any(not_biblio_indicators)

    def _looks_like_table_data(self, text: str) -> bool:
        table_indicators = [
            bool(re.search(r'\d+[\s,]*—Ç\.—Ä\.', text)),
            bool(re.search(r'\d+[\s,]*—Ä—É–±', text)),
            bool(re.search(r'\d+[\s,]*%', text)),
            len(text) < 50 and any(char.isdigit() for char in text),
            any(word in text.lower() for word in ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∑–∞–∫—É–ø–∫–∞', '—Ä–∞—Å—Ö–æ–¥', '–¥–æ—Ö–æ–¥']),
        ]
        return any(table_indicators)

    def _is_bibliography_entry(self, text: str) -> bool:
        if not text or not text.strip():
            return False

        text_lower = text.lower().strip()

        if any(keyword in text_lower for keyword in self.biblio_keywords):
            return False
        if '...' in text:
            return False
        if len(text) < 20:
            return False

        starts_with_number = any(text.strip().startswith(f"{i}.") for i in range(1, 100))
        starts_with_bracket = re.match(r'^\[\d+\]', text.strip())
        has_year = bool(re.search(r'\b(19|20)\d{2}\b', text))

        has_biblio_keywords = any(keyword in text_lower for keyword in [
            '–∏–∑–¥-–≤–æ', '–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–∂—É—Ä–Ω–∞–ª', '—Ç.', '–≤—ã–ø.', '—Å.', '—Å—Ç—Ä.', '—Å—Å.',
            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º–∏—è', '–Ω–∞—É–∫',
            '–∏–∑–¥–∞–Ω–∏–µ', '–º–æ–Ω–æ–≥—Ä–∞—Ñ–∏—è', '—É—á–µ–±–Ω–∏–∫', '–ø–æ—Å–æ–±–∏–µ', '—Å—Ç–∞—Ç—å—è',
            '–º.:', '—Å–ø–±.:', '–∫–∏–µ–≤:', '–º–∏–Ω—Å–∫:',
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥'
        ])

        has_comma_and_year = (',' in text and bool(re.search(r'\b(19|20)\d{2}\b', text)))
        punctuation_count = text.count('.') + text.count(',')
        has_punctuation = punctuation_count >= 3
        has_abbreviations = any(abbr in text for abbr in ['—Ç.', '–≤—ã–ø.', '—Å.', '—Å—Å.', '–≥.'])
        reasonable_length = 30 < len(text) < 800

        strong_indicators = [
            starts_with_number,
            bool(starts_with_bracket),
            has_year and has_punctuation,
            has_biblio_keywords and has_year,
            has_comma_and_year and has_punctuation
        ]

        weak_indicators = [
            has_year,
            has_biblio_keywords,
            has_punctuation,
            has_abbreviations
        ]

        is_bibliography = (any(strong_indicators) or (sum(weak_indicators) >= 2)) and reasonable_length

        if is_bibliography and (starts_with_number or starts_with_bracket):
            print(f"   –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –∫–∞–∫ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è: {text[:70]}...")

        return is_bibliography

    def check_citations_vs_bibliography(self, citations: List[str], bibliography_blocks: List[TextBlock]) -> Dict[
        str, Any]:
        if not bibliography_blocks:
            return {
                'valid_references': [],
                'missing_references': citations,
                'valid_count': 0,
                'missing_count': len(citations),
                'bibliography_found': False
            }

        bibliography_entries_count = len(bibliography_blocks)
        print(f"–ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç {bibliography_entries_count} –∑–∞–ø–∏—Å–µ–π")

        valid_references = []
        missing_references = []

        for citation in citations:
            try:
                citation_num = int(citation)
                if 1 <= citation_num <= bibliography_entries_count:
                    valid_references.append(citation)
                    print(f"   –¶–∏—Ç–∞—Ç–∞ [{citation}] –≤–∞–ª–∏–¥–Ω–∞ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 1..{bibliography_entries_count})")
                else:
                    missing_references.append(citation)
                    print(f"   –¶–∏—Ç–∞—Ç–∞ [{citation}] –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏ (1..{bibliography_entries_count})")
            except ValueError:
                missing_references.append(citation)
                print(f"   –ù–µ—á–∏—Å–ª–æ–≤–∞—è —Ü–∏—Ç–∞—Ç–∞ [{citation}] –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

        return {
            'valid_references': valid_references,
            'missing_references': missing_references,
            'valid_count': len(valid_references),
            'missing_count': len(missing_references),
            'bibliography_found': True
        }

    def _search_semantic_scholar(self, query: str) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –≤ Semantic Scholar API"""
        try:
            headers = {}
            if self.config.SEMANTIC_SCHOLAR_API_KEY:
                headers['x-api-key'] = self.config.SEMANTIC_SCHOLAR_API_KEY

            params = {
                'query': query,
                'limit': 3,
                'fields': 'title,authors,year,venue,doi,url'
            }

            response = self.session.get(
                'https://api.semanticscholar.org/graph/v1/paper/search',
                params=params,
                headers=headers,
                timeout=self.config.REQUEST_TIMEOUT
            )

            if response.status_code == 200:
                data = response.json()
                return self._parse_semantic_scholar_results(data)
            else:
                self.logger.warning(f"Semantic Scholar API returned status {response.status_code}")

        except Exception as e:
            self.logger.error(f"Semantic Scholar API error: {e}")

        return []

    def _filter_best_result(self, results: List[SearchResult], original_query: str) -> Optional[SearchResult]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É"""
        if not results:
            return None

        # –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        sorted_results = sorted(results, key=lambda x: x.confidence, reverse=True)

        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        query_words = set(original_query.lower().split())

        for result in sorted_results:
            if result.title:
                title_words = set(result.title.lower().split())
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
                common_words = query_words.intersection(title_words)
                if len(common_words) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 –æ–±—â–∏—Ö —Å–ª–æ–≤–∞
                    return result

        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–π —É–≤–µ—Ä–µ–Ω–Ω—ã–π
        return sorted_results[0] if sorted_results else None

    def _is_relevant_result(self, result: SearchResult, original_text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –∑–∞–ø–∏—Å–∏"""
        original_lower = original_text.lower()
        result_title = result.title.lower() if result.title else ""

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∏ —Ç–µ—Å—Ç
        key_phrases = [
            '—Ç–æ–ª—Å—Ç–æ–π', '–≤–æ–π–Ω–∞ –∏ –º–∏—Ä',  # –î–ª—è –¢–æ–ª—Å—Ç–æ–≥–æ
            '—ç–∫–æ–Ω–æ–º–∏–∫', '–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö',  # –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∫–∏
            '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–∫–Ω—É—Ç—Å–µ–Ω',  # –î–ª—è ML
            '–±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω', '–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–æ'  # –î–ª—è –±–∏–∑–Ω–µ—Å–∞
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        for phrase in key_phrases:
            if phrase in original_lower and phrase in result_title:
                return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–≤—Ç–æ—Ä–æ–≤
        if result.authors:
            for author in result.authors:
                author_lower = author.lower()
                if any(author_word in original_lower for author_word in author_lower.split()):
                    return True

        return False

    def _enhance_single_entry(self, entry: BibliographyEntry) -> BibliographyEntry:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        search_queries = self._generate_search_queries(entry.text)

        if entry.online_metadata is None:
            entry.online_metadata = {}
        entry.online_metadata['search_queries_used'] = search_queries

        best_relevant_result = None
        best_confidence = 0.0

        for query in search_queries:
            print(f"      üîé –ü–æ–∏—Å–∫: '{query}'")
            try:
                results = self.online_searcher.search_publication(query)

                if results:
                    for result in results:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                        if self._is_relevant_result(result, entry.text):
                            if result.confidence > best_confidence:
                                best_relevant_result = result
                                best_confidence = result.confidence
                                print(f"      –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.2f})")

                                if result.confidence > 0.8:
                                    break
                    else:
                        print(f"      –ù–∞–π–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ")
                else:
                    print(f"      –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è: {query}")

            except Exception as e:
                print(f"      –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
                continue

        if best_relevant_result and best_confidence > 0.3:
            entry.online_metadata = self._format_online_metadata(best_relevant_result)
            entry.is_verified = True
            entry.enhancement_confidence = best_confidence
            print(f"      –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é: {best_confidence:.2f}")
        else:
            print(f"      –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            # –ú–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –∏–¥–µ–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
            if results and not best_relevant_result:
                fallback_result = results[0]
                entry.online_metadata = self._format_online_metadata(fallback_result)
                entry.is_verified = False  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π
                entry.enhancement_confidence = fallback_result.confidence * 0.5  # –ü–æ–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                print(f"      –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {fallback_result.confidence:.2f})")

        return entry